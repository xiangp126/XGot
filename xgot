#!/usr/bin/env python3
from urllib import request, parse, error
from time import sleep
import math
import socket
import logging
import os
import re
import json
from common import *
from multiprocessing import Process

title = "hello"
output_dir = "./downloads"
suffix = "ts"
need_merge = True
headers = {'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}
debug = False

parts = []
output_filepath = ""

def print_url(urls):
    for url in urls:
        print(url)

""" split list
@urls: the list to split
@num_process: number of process want to use
"""
def chunks(urls, num_process):
    ret = []
    tot_len = len(urls)
    each = math.ceil(tot_len / num_process)
    for loop in range(0, num_process):
        start = loop * each
        end_pass = start + each
        ret.append(urls[start:end_pass])
    return [each, ret]

def pre_download(urls, title, suffix, output_dir, merge):
    assert urls
    global output_filepath

    for i, url in enumerate(urls):
        filename = '%s_%02d.%s' % (title, i, suffix)
        filepath = os.path.join(output_dir, filename)
        parts.append(filepath)

    output_filename = get_output_filename(urls, title, suffix, output_dir, merge)
    output_filepath = os.path.join(output_dir, output_filename)

def start_work(**kwargs):
    title_patt = r'<title>(.+?)</title>'
    m3u8_patt = r'(https?://[^;"\'\\]+' + '\.m3u8?' + r'[^;"\'\\]*)'

    page = get_content(url, headers)
    hit = re.findall(m3u8_patt, page)
    if hit is None:
        print("Could not extract first m3u8 url")
        return
    first_m3u8_url = hit[0]

    # retrieve title
    hit = re.search(title_patt, page)
    if hit != None:
        global title
        title = hit.group(1)
    print(title)

    m3u8_url_list = real_m3u8_extractor(first_m3u8_url, headers)
    m3u8_url = m3u8_url_list[0]
    if m3u8_url is None:
        print("Could not extract real m3u8 url")
        return
    print(m3u8_url)

    # retrieve real TS url
    urls = general_m3u8_extractor(m3u8_url, headers)
    item_urls = len(urls)
    num_process = 50
    process_list = []

    if debug:
        print_url(urls)

    print("\nUsed Number of Process: %d." %num_process)
    print("Total Sliced Number of TS: %d.\n" %len(urls))

    # index_base = 0
    pre_download(urls, title, suffix, output_dir, need_merge)
    # i = 0
    # each, _urls = chunks(urls, num_process)
    # for slice_urls in _urls:
        # new_p = Process(target = download_urls, args = (slice_urls, title,
                        # suffix, 0, parts, output_dir, need_merge, headers,
                        # index_base))
        # process_list.append(new_p)
        # index_base += each

    # for p in process_list:
        # p.start()
    # for p in process_list:
        # p.join()

    # download_urls(slice_urls, title, suffix, 0, output_dir = output_dir,
                  # merge = need_merge, parts = parts, base = index_base, **kwargs)
    tackle_slice_of_ts(parts, output_filepath, suffix, need_merge)

if __name__ == '__main__':
    _srcdir = '%s/src/' %os.path.dirname(os.path.realpath(__file__))
    _filepath = os.path.dirname(sys.argv[0])
    sys.path.insert(1, os.path.join(_filepath, _srcdir))

    global url
    url = sys.argv[1]
    start_work()
